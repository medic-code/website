---
title: 'The complete guide to understanding linear regression'
date: '2023-05-24'
slug: 'linearRegression'
lastUpdated: '2023-05-24'
type: 'technicalBlog'
canonicalUrl: 'https://www.aaron-smith.dev/linearRegression5mins'
---

In this short article, we will cover the basics of linear regression in plain
English without mathematical equations. This article aims to have some
intuition about what linear regression is. By the end of this article, you will
understand the linear regression model when to use it, and
its role within machine learning.

## What is linear regression?

Linear regression is a statistical method that models relationships between one
set of data to another by using a line of best fit. First, we can hypothesize that
there's a relationship between some inputs and output data. Linear regression
finds the line of best fit we can then use to predict an output from new input
data.

Say we hypothetically have data on the number of hours studying for an exam and
the eventual exam scores for students. By plotting the study hours against the
exam score, we can find a relationship between the two. In this case, our input
data is the hours studied, and the output is the exam score. We want
to understand the relationship between the two by drawing a line through that
data. This line can predict the exam score based on how many hours
a student studies for the exam.

In the image below, we plot the hours studied by students against the exam score
they got. By using linear regression, we can plot a line that best fits the data
and therefore allows us to make predictions.

<Image
  src="/Images/LinearReg/1.svg"
  alt="An example scatter plot and linear regression model"
  layout="responsive"
  width={200}
  height={100}
/>

Linear regression is a fundamental algorithm in machine learning. Specifically,
it is called a supervised learning algorithm. By supervised, we mean that we
have a trained set of data, and using linear regression, we can learn the best way
to create a model we can use to make predictions about new data.

In machine learning, linear regression can identify the essential inputs for a
given output which is important in machine learning models. It also
serves as a baseline model to compare against more complex models. Additionally,
we evaluate machine learning models using the same techniques for linear
regression. We can therefore use linear regression as a foundation to understand
some fundamental concepts in machine learning.

## Defining the terms

Now that we know what linear regression is, let's break the phrase linear
regression down. The regression part refers to a class of algorithms that make
outputs a continuous number that is our prediction based on input data. The
commonest regression algorithm is linear regression.

The linear aspect refers to the fact that we assume the data we use is linear
and can make a prediction based on that relationship. But what does it mean to
say a relationship is linear?

For our example data above to have a linear relationship, the exam score increases as
the number of hours increases. If that is the case generally, then we have a
linear relationship. Another way to say it is that we call this linear data when
there is a constant rate of change between input (in this case, the number of
hours studied) and output (exam score).

## How does linear regression work?

In linear regression, we assume the data is linear first. We can then create a
**model** represented by a straight line to best fit the data to make
predictions.

Using mathematics, we can find an equation of a line (or linear equation) that
best fits our dataset. This equation represents our model, and we can
use that equation to predict the outcome of unseen data.

For example, say in the number of hours studied and exam grades example; we
could have someone who puts in 500 hours of study time. We have yet to see a
person put that much effort into studying. Still, with linear regression, we can
predict the exam score for this person.

<Image
  src="/Images/LinearReg/2.svg"
  alt="Predicting the exam score from a student who does 150hrs studying"
  layout="responsive"
  width={200}
  height={600}
/>

Now, studying hours are not the only factor in exam scores. Yes, that's
true; the model is simplistic, but to explain the concepts, let's work with it.

Linear regression in the real world is often more complicated than the example
above about studying and exam scores. In the case of one input (study hours) and
one output(exam score), we call this **simple linear regression**. The term we
use to describe a linear regression model with multiple inputs to output is
**multiple linear regression**.

To illustrate the linear regression model differently, say we put some dots on a
page that looks like a line. Think of the points as magnets. We place a rod near
the dots; in this case, the rod will be pulled until there's equilibrium such
that it's closest to all points. In this circumstance, the rod is drawn to a
balance is how we can best make predictions for that data.

## Solutions

### Least squares method

Now you might be thinking, how do we create a model that best makes predictions
for our linear data? We do this by finding the equation using mathematics. The
method to do this is called the least squares method.

To understand, let's first think about how to find a line of best fit.
We could line through our data and compare the model's predicted output
against our actual data. If the two are close, we see the model fits the data
well; if the two are far apart, the line does not represent the data very well.

We can find the differences between each predicted outcome from the line drawn
and the actual outcome for all of our datasets; we call the difference between
the predicted outcome and actual outcome a **residual**. To get the best-fit
we can add all these residuals up the equation for our model and then say that
to get the best line that models the data; we want this total to be as small as
possible. We can use formulas to find the parts of the linear equation
when that sum of residuals is at a minimum. We can then put the data set into
these equations and find out the line of best fit.

<Image
  src="/Images/LinearReg/3.svg"
  alt="Calculating the difference between predicted and actual values"
  layout="responsive"
  width={200}
  height={600}
/>

Finding the equation representing our linear regression model is so simple that
we don't need fancy machine-learning algorithms. Using some algebra and some
formulas, we can calculate the line equation for our dataset, which means the
sum of the residuals is minimized. We call this a **closed-form solution** for
our dataset; we can just put numbers into a formula and get an equation
representing the linear regression model.

There are other ways to find the best linear equation to fit the data. In the
next section below, we will discuss a more general way by using an algorithm to
model data which we can use to find the best fit linear equation. We call this
**gradient descent**. Gradient descent is used a lot in machine learning, and
the simplicity of linear regression makes it easier to understand.

### Gradient Descent

The gradient descent algorithm is a way to minimise the difference between our
data and our predicted values from our data by using mathematics to guide us to
the best solution iteratively. To say it another way, we want to learn the best
linear equation that fits our data.

First, we start with a random guess that we choose as to the equation we're
using for our model. The guess can be any linear equation, the gradient descent
algorithm looks at how good that equation is using mathematics. It then makes a
slight change to that equation and tests whether it would make the equation a
better fit for our data. By better, we mean the equation changes so we get
closer to minimising the error between our predicted and actual outcomes for the
whole dataset.

The gradient part of the term refers to a mathematics term that can detect how
quickly something changes. By detecting change, we can know when the difference
between two things is minimal. In our case, we want to know the most negligible
difference between predicted and actual data.

We mentioned above in the previous section about minimising the error between a
predicted and actual outcome. That is when we know we have a good model.
Gradient descent will tell us when we are getting closer to minimising this
error for all our data. The equation that starts as a guess gets updated a
little bit at a time, and using the revised equation, we can test to see if
we're getting closer to our goal.

In the image below, think of gradient descent, picking this line to model our
data. It's obvious, looking at the data, that this is a poor choice to start
with.

<Image
  src="/Images/LinearReg/4.svg"
  alt="Illustrating a first guess attempt at the best fit linear equation"
  layout="responsive"
  width={200}
  height={200}
/>

We mentioned above that we updated the equation in gradient descent; in the
image below, you can see the equation fits our data better.

<Image
  src="/Images/LinearReg/5.svg"
  alt="Illustrating a second guess attempt at the best fit linear equation"
  layout="responsive"
  width={200}
  height={200}
/>

We update this equation over and over until we have an equation such that the
difference between predicted outcomes using that equation and actual outcomes
are the minimum. When we have that, we have the line of best fit for our
dataset.

Now we understand how we do linear regression, its worth taking the time to
understand the assumptions of the model and the strengths and weaknesses of
the model.

## Model assumptions and context

When we decided to use Linear regression to model our data, there are a number
of assumptions to consider when we make that decision. First linear regression
assumes that the data generall has a linear relationship between the input data
and the output. The model's accuracy will be affected a large amount if the data
is not linear.

Additionally, the input data must be independent of each other when there are multiple inputs
to an output. Having input data influenced by each other (or correlated) leads
to poor accuracy in the model.

Lastly, linear regression assumes no outliers in the data. Having outliers in
the data can heavily affect the model's accuracy.

Considering the above, it's still okay to use linear regression when the data
does not fulfill all the assumptions, but knowing what they are will help you
decide if linear regression is going to model that is useful. Additionally,
there are ways to change the data to suit these assumptions, for example, by
removing outliers.

The power of linear regression is that it is easy to compute and works well with
large datasets. Unlike other models, it's also straightforward to see how the
model made its predictions.

However, the problem with linear regression is that the data must have a linear
relationship to be useful in making predictions. Additionally, suppose the data
or some inputs to the model are too linear (or correlated, as we say). In that
case, it can make the model unreliable. It's also sensitive to outliers; our
model will change more than we would like to accommodate if there are outliers.

## Summary

1. Linear regression is a method to model the relationship between input and
   output data using a line of best fit between the data.
2. Linear regression is a supervised machine learning algorithm used to compare
   against other machine learning models and provide insights into what inputs
   are most useful to the model.
3. We can create the model using the least squares method or the gradient
   descent algorithm.
4. Using linear regression, we assume the following, that the data has a linear relationship,
   there are few outliers, and that the input data is independent.
5. Linear regression is easy to compute and shows high predictability
6. Linear regression is sensitive to outliers and only works for linear data.
   When the data is heavily correlated, the model accuracy is reduced.

## Further Resources
